---
title: "Scraping Wikipedia articles for each year of the Tour de France"
output: html_notebook
---

# SETUP
Load libraries
```{r setup}
library(rvest)
library(tidyverse)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
```

# SCRAPE TEXT FROM URL FOR EACH ARTICLE
Tibble with row = article, col = URL for article.
```{r url_list}
years <- seq(1903, 2019, 1)
url_base <- "https://en.wikipedia.org/wiki/"
url_end <- "_Tour_de_France"
urls <- years %>%
  paste0(url_base, . , url_end)

data_init <- tibble(year = years, url_path = urls)

data_init[1:20,]
```

Function for scraping text from wikipage.
```{r get_text}
get_text <- function(url){
  text <- url %>%
    read_html() %>%
    html_node("#mw-content-text > div") %>%
    html_nodes("p") %>% 
    html_text()
}
```

Scrape text for every year and put in tidytext format (one row per word). Save .csv to data folder so don't need to keep repeating scraping.
```{r scrape}
text_raw <- data_init %>%
  mutate(text = purrr::map(url_path, possibly(get_text, "no data"))) %>%
  filter(text != "no data") %>%
  mutate(text = purrr::map(text, ~tibble(text = .x))) %>%
  select(year, text) %>%
  unnest() %>%
  mutate(text = str_remove_all(text, regex("\\[\\d+\\]")),
         text = str_remove_all(text, regex("\\n"))) 

write_csv(text_raw, "data/text_raw.csv")
```

